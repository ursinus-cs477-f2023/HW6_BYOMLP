{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d58f1939",
   "metadata": {},
   "source": [
    "# CS 477 HW 7: Gradient Descent on Neural Networks\n",
    "## Chris Tralie\n",
    "\n",
    "This is a simple test to make sure our neural network engine is able to separate the inside of a circle from the outside, which would not work with logistic regression over the two dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8410cd7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "import IPython.display as ipd\n",
    "from neuralnet import *\n",
    "from layers import *\n",
    "from losses import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a360fb89",
   "metadata": {},
   "source": [
    "First, let's generate the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "435b027e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_disc_points(N):\n",
    "    X = np.random.randn(N, 2)\n",
    "    d = np.sqrt(np.sum(X**2, axis=1))\n",
    "    ys = np.array(d < 1, dtype=float)\n",
    "    X[ys == 0, :] *= 1.1 # Put a small gap between inner and outer points\n",
    "    return X, ys\n",
    "\n",
    "np.random.seed(0)\n",
    "X, ys = get_disc_points(1000)\n",
    "plt.scatter(X[ys==0, 0], X[ys==0, 1])\n",
    "plt.scatter(X[ys==1, 0], X[ys==1, 1])\n",
    "plt.axis(\"equal\")\n",
    "plt.title(\"Intial Disc Data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7adb816",
   "metadata": {},
   "source": [
    "Here's some code we'll use to plot the second to last layer with two neurons, which will indicate how well the last linear separator can do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b45d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_2d_separator_predictions(X1, X2, a, b, c, draw_lines=False):\n",
    "    \"\"\"\n",
    "    Plot the performance of a 2D linear separator on a set of binary labeled data.\n",
    "    This is applicable to any layer which takes two neurons to one\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X1: ndarray(N1, 2)\n",
    "        Input coordinates for class 1\n",
    "    X2: ndarray(N, 2)\n",
    "        Input coordinates for class 2\n",
    "    a: float\n",
    "        Weight for first coordinate\n",
    "    b: float\n",
    "        Weight for second coordinate\n",
    "    c: float\n",
    "        Bias\n",
    "    \"\"\"\n",
    "    plot = [plt.scatter(X1[:, 0], X1[:, 1], 1, c='C0')]\n",
    "    plot.append(plt.scatter(X2[:, 0], X2[:, 1], 1, c='C1'))\n",
    "    X = np.concatenate((X1, X2), axis=0)\n",
    "    xmin = np.min(X, axis=0)\n",
    "    xmax = np.max(X, axis=0)\n",
    "    iv = max(xmax[1]-xmin[1], xmax[0]-xmin[0])\n",
    "    \n",
    "    p0 = -c*np.array([a, b])/(a**2 + b**2)\n",
    "    v = np.array([-b, a])\n",
    "    mag = np.sqrt(np.sum(v**2))\n",
    "    wrong = 0\n",
    "    if mag > 0:\n",
    "        v = v/mag\n",
    "        p = p0 - 2*iv*v\n",
    "        q = p0 + 2*iv*v\n",
    "        plot += plt.plot([p[0], q[0]], [p[1], q[1]], c='k', linestyle='--')\n",
    "        rg = xmax[0] - xmin[0]\n",
    "        plt.xlim([xmin[0]-0.2*rg, xmax[0]+0.2*rg])\n",
    "        rg = xmax[1] - xmin[1]\n",
    "        plt.ylim([xmin[1]-0.2*rg, xmax[1]+0.2*rg])\n",
    "\n",
    "        wrong = 0\n",
    "        for x in X1:\n",
    "            proj = p0 + np.sum(v*(x-p0))*v\n",
    "            y = a*x[0] + b*x[1] + c\n",
    "            if draw_lines:\n",
    "                plot += plt.plot([x[0], proj[0]], [x[1], proj[1]], c='C0', linewidth=1, linestyle='--')\n",
    "            if y > 0:\n",
    "                plot.append(plt.scatter([x[0]], [x[1]], 1, c='C0', marker='x'))\n",
    "                wrong += 1\n",
    "        for x in X2:\n",
    "            proj = p0 + np.sum(v*(x-p0))*v\n",
    "            if draw_lines:\n",
    "                plot += plt.plot([x[0], proj[0]], [x[1], proj[1]], c='C1', linewidth=1, linestyle='--')\n",
    "            y = a*x[0] + b*x[1] + c\n",
    "            if y < 0:\n",
    "                plot.append(plt.scatter([x[0]], [x[1]], 1, c='C1', marker='x'))\n",
    "                wrong += 1\n",
    "    total = X1.shape[0] + X2.shape[0]\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plot.append(plt.text(0.5, 1.01,\n",
    "                        \"{} / {} Correct ({:.1f}%)\".format(total-wrong, total, 100*(total-wrong)/total),\n",
    "                        horizontalalignment='center', verticalalignment='bottom',\n",
    "                        transform=plt.gca().transAxes, size='xx-large'))\n",
    "    return plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e0dc605",
   "metadata": {},
   "source": [
    "Finally, let's setup a neural network and train it!  We'll put 100 neurons in the first hidden layer, followed by 2 neurons, followed by a single neuron with the logistic activation.  Since the last hidden layer has 2 neurons, we can the coordinates on the data mapped through it to see how well it's being separated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed07754a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_animation=True\n",
    "np.random.seed(3)\n",
    "\n",
    "nn = NeuralNet(2, logistic_est_loss_deriv) # Input is in 2 dimensions, and we want to use logistic loss\n",
    "nn.add_layer(100,  leaky_relu, leaky_relu_deriv) # First layer is 100 dimensions with a leaky ReLU\n",
    "nn.add_layer(2, leaky_relu, leaky_relu_deriv) # Second layer is 2 dimensions with a leaky ReLU\n",
    "nn.add_layer(1, logistic, None) # Last layer is the logistic function.  Its derivative is handled separately\n",
    "\n",
    "n_iters = 30\n",
    "alpha = 0.001\n",
    "losses = []\n",
    "\n",
    "fig = plt.figure(figsize=(12, 6))\n",
    "frames = []\n",
    "for it in range(n_iters):\n",
    "    loss = 0\n",
    "    X1 = []\n",
    "    X2 = []\n",
    "    for k in range(X.shape[0]):\n",
    "        y_est = nn.forward(X[k, :])\n",
    "        loss += logistic_est_loss(y_est, ys[k])\n",
    "        if ys[k] == 0:\n",
    "            X1.append(nn.h[2])\n",
    "        else:\n",
    "            X2.append(nn.h[2])\n",
    "    print(\"Iteration {} Loss {}\".format(it, loss))\n",
    "    losses.append(loss)\n",
    "    X1 = np.array(X1)\n",
    "    X2 = np.array(X2)\n",
    "\n",
    "    # Plot Result\n",
    "    # Get the 2D linear separator from the weights/bias in the last layer\n",
    "    a, b = nn.Ws[-1].flatten() \n",
    "    c = nn.bs[-1][0]\n",
    "    plt.subplot(121)\n",
    "    plot = plot_2d_separator_predictions(X1, X2, a, b, c)\n",
    "    plt.gca().set_facecolor(\"white\")\n",
    "    plt.subplot(122)\n",
    "    plot += plt.plot(losses, c='C0')\n",
    "    plt.xlabel(\"Iteration\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.xlim([0, n_iters])\n",
    "    plt.ylim([0, np.max(losses)])\n",
    "    plot.append(plt.text(0.5, 1.01, \"Iteration {} Loss {:.3f}\".format(it, loss[0]),\n",
    "                        horizontalalignment='center', verticalalignment='bottom',\n",
    "                        transform=plt.gca().transAxes, size='xx-large'))\n",
    "    plt.gca().set_facecolor(\"white\")\n",
    "    frames.append(plot)\n",
    "    \n",
    "    # Stochastic gradient descent\n",
    "    for k in np.random.permutation(X.shape[0]):\n",
    "        nn.backprop_descent(X[k, :], ys[k], alpha)\n",
    "\n",
    "ani = animation.ArtistAnimation(fig, frames, interval=200, blit=True, repeat_delay=1000)\n",
    "ani.save(\"result.gif\", dpi=200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8badb887",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
